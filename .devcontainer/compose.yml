services:

  devcontainer:
    build:
      context: .
      dockerfile: Dockerfile.python
    volumes:
      - ../..:/workspaces:cached      
    network_mode: service:ollama-service
    environment:
      - OLLAMA_KEEP_ALIVE=-1
      - OLLAMA_DEFAULT_KEEP_ALIVE=-1
    command: sleep infinity

  ollama-service:
    image: ollama/ollama:latest # a best practice is to use fixed tag for the image
    volumes:
      - ./ollama:/root/.ollama
    ports:
      - 11434:11434
    # Ollama can run with GPU acceleration inside Docker containers for Nvivia GPUs
    #deploy:
    #  resources:
    #    reservations:
    #      devices:
    #        - driver: nvidia
    #          count: 1
    #          capabilities: [gpu]


  download-qwen-data:
    image: curlimages/curl:8.6.0
    entrypoint: ["curl", "ollama-service:11434/api/pull", "-d", "{\"name\": \"qwen:0.5b\"}"]
    depends_on:
      ollama-service:
        condition: service_started

  download-nemotron-mini-data:
    image: curlimages/curl:8.6.0
    entrypoint: ["curl", "ollama-service:11434/api/pull", "-d", "{\"name\": \"nemotron-mini\"}"]
    depends_on:
      ollama-service:
        condition: service_started


  download-nemotron-mini-local-data:
    image: curlimages/curl:8.6.0
    entrypoint: ["curl", "host.docker.internal:11434/api/pull", "-d", "{\"name\": \"nemotron-mini\"}"]
    depends_on:
      ollama-service:
        condition: service_started

  download-llama-3-2-data:
    image: curlimages/curl:8.6.0
    entrypoint: ["curl", "host.docker.internal:11434/api/pull", "-d", "{\"name\": \"llama3.2:1b\"}"]
    depends_on:
      ollama-service:
        condition: service_started

  download-qwen-2-5-data:
    image: curlimages/curl:8.6.0
    entrypoint: ["curl", "host.docker.internal:11434/api/pull", "-d", "{\"name\": \"qwen2.5-coder:1.5b\"}"]
    depends_on:
      ollama-service:
        condition: service_started
